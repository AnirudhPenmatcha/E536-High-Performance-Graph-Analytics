{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7VnfldOm0RdO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import sklearn.metrics as metrics\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import SAGEConv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid(root='Cora', name='Cora')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY564AYe1x7W",
        "outputId": "d8f6b3e6-68b9-429c-859c-6925e796fb18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Testing parameters in GCN"
      ],
      "metadata": {
        "id": "o5APymoG_6zY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Parameter hidden layer dimensions: 2, 4, 8, 16, 64, 512"
      ],
      "metadata": {
        "id": "1MpU127jAJFg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdD4tGAB3l-p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Net, self).__init__()\n",
        "        # you can change convolutions here\n",
        "        #self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = SAGEConv(16, dataset.num_classes)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_size)\n",
        "        self.conv2 = GCNConv(hidden_size, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_dimensions = [2,4,8,16,64,512]\n",
        "\n",
        "for dimension in hidden_layer_dimensions: # Looping through a set of different hidden layer sizes\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = Net(hidden_size = dimension).to(device)\n",
        "  data = dataset[0].to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "  model.train()\n",
        "  print(f\"####Training GCN with a {dimension} hidden layer dimension...####\")\n",
        "  for epoch in range(100):\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data)\n",
        "      loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      _, pred = model(data).max(dim=1)\n",
        "      correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "      acc = correct / data.train_mask.sum().item()\n",
        "      print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))\n",
        "\n",
        "  _, pred = model(data).max(dim=1)\n",
        "  correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "  acc = correct / data.test_mask.sum().item()\n",
        "  print('####Test Accuracy with a', dimension, 'hidden layer dimension: {:.4f}####'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nNVB8WG4rgz",
        "outputId": "4064e2c8-047f-4a4a-9eb8-4937d770606e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####Training GCN with 2 hidden layers...####\n",
            "Epoch: 0, Accuracy: 0.2357\n",
            "Epoch: 1, Accuracy: 0.2929\n",
            "Epoch: 2, Accuracy: 0.2500\n",
            "Epoch: 3, Accuracy: 0.2571\n",
            "Epoch: 4, Accuracy: 0.2714\n",
            "Epoch: 5, Accuracy: 0.2786\n",
            "Epoch: 6, Accuracy: 0.3071\n",
            "Epoch: 7, Accuracy: 0.3286\n",
            "Epoch: 8, Accuracy: 0.3286\n",
            "Epoch: 9, Accuracy: 0.3286\n",
            "Epoch: 10, Accuracy: 0.3357\n",
            "Epoch: 11, Accuracy: 0.3429\n",
            "Epoch: 12, Accuracy: 0.3429\n",
            "Epoch: 13, Accuracy: 0.3500\n",
            "Epoch: 14, Accuracy: 0.3571\n",
            "Epoch: 15, Accuracy: 0.3857\n",
            "Epoch: 16, Accuracy: 0.4500\n",
            "Epoch: 17, Accuracy: 0.5214\n",
            "Epoch: 18, Accuracy: 0.5286\n",
            "Epoch: 19, Accuracy: 0.5571\n",
            "Epoch: 20, Accuracy: 0.5714\n",
            "Epoch: 21, Accuracy: 0.5643\n",
            "Epoch: 22, Accuracy: 0.5714\n",
            "Epoch: 23, Accuracy: 0.5857\n",
            "Epoch: 24, Accuracy: 0.5929\n",
            "Epoch: 25, Accuracy: 0.6071\n",
            "Epoch: 26, Accuracy: 0.6071\n",
            "Epoch: 27, Accuracy: 0.6214\n",
            "Epoch: 28, Accuracy: 0.6500\n",
            "Epoch: 29, Accuracy: 0.6786\n",
            "Epoch: 30, Accuracy: 0.7000\n",
            "Epoch: 31, Accuracy: 0.7500\n",
            "Epoch: 32, Accuracy: 0.7571\n",
            "Epoch: 33, Accuracy: 0.7643\n",
            "Epoch: 34, Accuracy: 0.7714\n",
            "Epoch: 35, Accuracy: 0.7714\n",
            "Epoch: 36, Accuracy: 0.7714\n",
            "Epoch: 37, Accuracy: 0.7786\n",
            "Epoch: 38, Accuracy: 0.7857\n",
            "Epoch: 39, Accuracy: 0.7929\n",
            "Epoch: 40, Accuracy: 0.8000\n",
            "Epoch: 41, Accuracy: 0.8000\n",
            "Epoch: 42, Accuracy: 0.8000\n",
            "Epoch: 43, Accuracy: 0.8071\n",
            "Epoch: 44, Accuracy: 0.8071\n",
            "Epoch: 45, Accuracy: 0.8071\n",
            "Epoch: 46, Accuracy: 0.8071\n",
            "Epoch: 47, Accuracy: 0.8000\n",
            "Epoch: 48, Accuracy: 0.8000\n",
            "Epoch: 49, Accuracy: 0.8000\n",
            "Epoch: 50, Accuracy: 0.8071\n",
            "Epoch: 51, Accuracy: 0.8071\n",
            "Epoch: 52, Accuracy: 0.8071\n",
            "Epoch: 53, Accuracy: 0.8143\n",
            "Epoch: 54, Accuracy: 0.8357\n",
            "Epoch: 55, Accuracy: 0.8214\n",
            "Epoch: 56, Accuracy: 0.8214\n",
            "Epoch: 57, Accuracy: 0.8214\n",
            "Epoch: 58, Accuracy: 0.8429\n",
            "Epoch: 59, Accuracy: 0.8500\n",
            "Epoch: 60, Accuracy: 0.8429\n",
            "Epoch: 61, Accuracy: 0.8429\n",
            "Epoch: 62, Accuracy: 0.8429\n",
            "Epoch: 63, Accuracy: 0.8429\n",
            "Epoch: 64, Accuracy: 0.8429\n",
            "Epoch: 65, Accuracy: 0.8500\n",
            "Epoch: 66, Accuracy: 0.8429\n",
            "Epoch: 67, Accuracy: 0.8429\n",
            "Epoch: 68, Accuracy: 0.8286\n",
            "Epoch: 69, Accuracy: 0.8214\n",
            "Epoch: 70, Accuracy: 0.8214\n",
            "Epoch: 71, Accuracy: 0.8214\n",
            "Epoch: 72, Accuracy: 0.8214\n",
            "Epoch: 73, Accuracy: 0.8214\n",
            "Epoch: 74, Accuracy: 0.8071\n",
            "Epoch: 75, Accuracy: 0.8071\n",
            "Epoch: 76, Accuracy: 0.7643\n",
            "Epoch: 77, Accuracy: 0.7143\n",
            "Epoch: 78, Accuracy: 0.7143\n",
            "Epoch: 79, Accuracy: 0.7143\n",
            "Epoch: 80, Accuracy: 0.7143\n",
            "Epoch: 81, Accuracy: 0.7143\n",
            "Epoch: 82, Accuracy: 0.7143\n",
            "Epoch: 83, Accuracy: 0.7143\n",
            "Epoch: 84, Accuracy: 0.7143\n",
            "Epoch: 85, Accuracy: 0.7143\n",
            "Epoch: 86, Accuracy: 0.7143\n",
            "Epoch: 87, Accuracy: 0.7143\n",
            "Epoch: 88, Accuracy: 0.7143\n",
            "Epoch: 89, Accuracy: 0.7786\n",
            "Epoch: 90, Accuracy: 0.8143\n",
            "Epoch: 91, Accuracy: 0.8143\n",
            "Epoch: 92, Accuracy: 0.8286\n",
            "Epoch: 93, Accuracy: 0.8286\n",
            "Epoch: 94, Accuracy: 0.8357\n",
            "Epoch: 95, Accuracy: 0.8429\n",
            "Epoch: 96, Accuracy: 0.8429\n",
            "Epoch: 97, Accuracy: 0.8429\n",
            "Epoch: 98, Accuracy: 0.8429\n",
            "Epoch: 99, Accuracy: 0.8571\n",
            "####Test Accuracy with 2 hidden layers: 0.5780####\n",
            "####Training GCN with 4 hidden layers...####\n",
            "Epoch: 0, Accuracy: 0.1714\n",
            "Epoch: 1, Accuracy: 0.1571\n",
            "Epoch: 2, Accuracy: 0.1571\n",
            "Epoch: 3, Accuracy: 0.1786\n",
            "Epoch: 4, Accuracy: 0.2071\n",
            "Epoch: 5, Accuracy: 0.2571\n",
            "Epoch: 6, Accuracy: 0.2643\n",
            "Epoch: 7, Accuracy: 0.2929\n",
            "Epoch: 8, Accuracy: 0.3214\n",
            "Epoch: 9, Accuracy: 0.3357\n",
            "Epoch: 10, Accuracy: 0.3714\n",
            "Epoch: 11, Accuracy: 0.4214\n",
            "Epoch: 12, Accuracy: 0.4429\n",
            "Epoch: 13, Accuracy: 0.4929\n",
            "Epoch: 14, Accuracy: 0.6143\n",
            "Epoch: 15, Accuracy: 0.6929\n",
            "Epoch: 16, Accuracy: 0.7357\n",
            "Epoch: 17, Accuracy: 0.7929\n",
            "Epoch: 18, Accuracy: 0.8214\n",
            "Epoch: 19, Accuracy: 0.8214\n",
            "Epoch: 20, Accuracy: 0.8286\n",
            "Epoch: 21, Accuracy: 0.8429\n",
            "Epoch: 22, Accuracy: 0.8571\n",
            "Epoch: 23, Accuracy: 0.8571\n",
            "Epoch: 24, Accuracy: 0.8643\n",
            "Epoch: 25, Accuracy: 0.8714\n",
            "Epoch: 26, Accuracy: 0.8786\n",
            "Epoch: 27, Accuracy: 0.8786\n",
            "Epoch: 28, Accuracy: 0.8857\n",
            "Epoch: 29, Accuracy: 0.8929\n",
            "Epoch: 30, Accuracy: 0.8929\n",
            "Epoch: 31, Accuracy: 0.8929\n",
            "Epoch: 32, Accuracy: 0.9000\n",
            "Epoch: 33, Accuracy: 0.9000\n",
            "Epoch: 34, Accuracy: 0.9000\n",
            "Epoch: 35, Accuracy: 0.9000\n",
            "Epoch: 36, Accuracy: 0.9000\n",
            "Epoch: 37, Accuracy: 0.9143\n",
            "Epoch: 38, Accuracy: 0.9214\n",
            "Epoch: 39, Accuracy: 0.9214\n",
            "Epoch: 40, Accuracy: 0.9214\n",
            "Epoch: 41, Accuracy: 0.9214\n",
            "Epoch: 42, Accuracy: 0.9214\n",
            "Epoch: 43, Accuracy: 0.9214\n",
            "Epoch: 44, Accuracy: 0.9214\n",
            "Epoch: 45, Accuracy: 0.9357\n",
            "Epoch: 46, Accuracy: 0.9357\n",
            "Epoch: 47, Accuracy: 0.9357\n",
            "Epoch: 48, Accuracy: 0.9357\n",
            "Epoch: 49, Accuracy: 0.9429\n",
            "Epoch: 50, Accuracy: 0.9500\n",
            "Epoch: 51, Accuracy: 0.9500\n",
            "Epoch: 52, Accuracy: 0.9643\n",
            "Epoch: 53, Accuracy: 0.9714\n",
            "Epoch: 54, Accuracy: 0.9714\n",
            "Epoch: 55, Accuracy: 0.9786\n",
            "Epoch: 56, Accuracy: 0.9786\n",
            "Epoch: 57, Accuracy: 0.9786\n",
            "Epoch: 58, Accuracy: 0.9786\n",
            "Epoch: 59, Accuracy: 0.9786\n",
            "Epoch: 60, Accuracy: 0.9786\n",
            "Epoch: 61, Accuracy: 0.9786\n",
            "Epoch: 62, Accuracy: 0.9786\n",
            "Epoch: 63, Accuracy: 0.9786\n",
            "Epoch: 64, Accuracy: 0.9786\n",
            "Epoch: 65, Accuracy: 0.9786\n",
            "Epoch: 66, Accuracy: 0.9857\n",
            "Epoch: 67, Accuracy: 0.9929\n",
            "Epoch: 68, Accuracy: 0.9929\n",
            "Epoch: 69, Accuracy: 0.9929\n",
            "Epoch: 70, Accuracy: 0.9929\n",
            "Epoch: 71, Accuracy: 0.9929\n",
            "Epoch: 72, Accuracy: 0.9929\n",
            "Epoch: 73, Accuracy: 0.9929\n",
            "Epoch: 74, Accuracy: 0.9929\n",
            "Epoch: 75, Accuracy: 0.9929\n",
            "Epoch: 76, Accuracy: 0.9929\n",
            "Epoch: 77, Accuracy: 0.9929\n",
            "Epoch: 78, Accuracy: 0.9929\n",
            "Epoch: 79, Accuracy: 0.9929\n",
            "Epoch: 80, Accuracy: 0.9929\n",
            "Epoch: 81, Accuracy: 0.9929\n",
            "Epoch: 82, Accuracy: 0.9929\n",
            "Epoch: 83, Accuracy: 0.9929\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 4 hidden layers: 0.7010####\n",
            "####Training GCN with 8 hidden layers...####\n",
            "Epoch: 0, Accuracy: 0.3857\n",
            "Epoch: 1, Accuracy: 0.5571\n",
            "Epoch: 2, Accuracy: 0.6357\n",
            "Epoch: 3, Accuracy: 0.6500\n",
            "Epoch: 4, Accuracy: 0.6786\n",
            "Epoch: 5, Accuracy: 0.6929\n",
            "Epoch: 6, Accuracy: 0.7571\n",
            "Epoch: 7, Accuracy: 0.7643\n",
            "Epoch: 8, Accuracy: 0.7929\n",
            "Epoch: 9, Accuracy: 0.8000\n",
            "Epoch: 10, Accuracy: 0.8143\n",
            "Epoch: 11, Accuracy: 0.8214\n",
            "Epoch: 12, Accuracy: 0.8357\n",
            "Epoch: 13, Accuracy: 0.8500\n",
            "Epoch: 14, Accuracy: 0.8714\n",
            "Epoch: 15, Accuracy: 0.8929\n",
            "Epoch: 16, Accuracy: 0.9000\n",
            "Epoch: 17, Accuracy: 0.9214\n",
            "Epoch: 18, Accuracy: 0.9500\n",
            "Epoch: 19, Accuracy: 0.9571\n",
            "Epoch: 20, Accuracy: 0.9714\n",
            "Epoch: 21, Accuracy: 0.9714\n",
            "Epoch: 22, Accuracy: 0.9786\n",
            "Epoch: 23, Accuracy: 0.9786\n",
            "Epoch: 24, Accuracy: 0.9929\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 8 hidden layers: 0.7900####\n",
            "####Training GCN with 16 hidden layers...####\n",
            "Epoch: 0, Accuracy: 0.7214\n",
            "Epoch: 1, Accuracy: 0.8786\n",
            "Epoch: 2, Accuracy: 0.8429\n",
            "Epoch: 3, Accuracy: 0.7929\n",
            "Epoch: 4, Accuracy: 0.7571\n",
            "Epoch: 5, Accuracy: 0.7929\n",
            "Epoch: 6, Accuracy: 0.8643\n",
            "Epoch: 7, Accuracy: 0.9357\n",
            "Epoch: 8, Accuracy: 0.9714\n",
            "Epoch: 9, Accuracy: 0.9786\n",
            "Epoch: 10, Accuracy: 0.9857\n",
            "Epoch: 11, Accuracy: 0.9857\n",
            "Epoch: 12, Accuracy: 0.9786\n",
            "Epoch: 13, Accuracy: 0.9786\n",
            "Epoch: 14, Accuracy: 0.9929\n",
            "Epoch: 15, Accuracy: 0.9929\n",
            "Epoch: 16, Accuracy: 0.9929\n",
            "Epoch: 17, Accuracy: 0.9929\n",
            "Epoch: 18, Accuracy: 0.9929\n",
            "Epoch: 19, Accuracy: 0.9929\n",
            "Epoch: 20, Accuracy: 1.0000\n",
            "Epoch: 21, Accuracy: 1.0000\n",
            "Epoch: 22, Accuracy: 1.0000\n",
            "Epoch: 23, Accuracy: 1.0000\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 16 hidden layers: 0.7980####\n",
            "####Training GCN with 64 hidden layers...####\n",
            "Epoch: 0, Accuracy: 0.9429\n",
            "Epoch: 1, Accuracy: 0.9714\n",
            "Epoch: 2, Accuracy: 0.9786\n",
            "Epoch: 3, Accuracy: 0.9786\n",
            "Epoch: 4, Accuracy: 0.9857\n",
            "Epoch: 5, Accuracy: 0.9857\n",
            "Epoch: 6, Accuracy: 0.9857\n",
            "Epoch: 7, Accuracy: 0.9857\n",
            "Epoch: 8, Accuracy: 0.9857\n",
            "Epoch: 9, Accuracy: 0.9929\n",
            "Epoch: 10, Accuracy: 1.0000\n",
            "Epoch: 11, Accuracy: 1.0000\n",
            "Epoch: 12, Accuracy: 1.0000\n",
            "Epoch: 13, Accuracy: 1.0000\n",
            "Epoch: 14, Accuracy: 1.0000\n",
            "Epoch: 15, Accuracy: 1.0000\n",
            "Epoch: 16, Accuracy: 1.0000\n",
            "Epoch: 17, Accuracy: 1.0000\n",
            "Epoch: 18, Accuracy: 1.0000\n",
            "Epoch: 19, Accuracy: 1.0000\n",
            "Epoch: 20, Accuracy: 1.0000\n",
            "Epoch: 21, Accuracy: 1.0000\n",
            "Epoch: 22, Accuracy: 1.0000\n",
            "Epoch: 23, Accuracy: 1.0000\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 64 hidden layers: 0.8100####\n",
            "####Training GCN with 512 hidden layers...####\n",
            "Epoch: 0, Accuracy: 0.9500\n",
            "Epoch: 1, Accuracy: 0.9786\n",
            "Epoch: 2, Accuracy: 0.9857\n",
            "Epoch: 3, Accuracy: 1.0000\n",
            "Epoch: 4, Accuracy: 1.0000\n",
            "Epoch: 5, Accuracy: 1.0000\n",
            "Epoch: 6, Accuracy: 1.0000\n",
            "Epoch: 7, Accuracy: 1.0000\n",
            "Epoch: 8, Accuracy: 1.0000\n",
            "Epoch: 9, Accuracy: 1.0000\n",
            "Epoch: 10, Accuracy: 1.0000\n",
            "Epoch: 11, Accuracy: 1.0000\n",
            "Epoch: 12, Accuracy: 1.0000\n",
            "Epoch: 13, Accuracy: 1.0000\n",
            "Epoch: 14, Accuracy: 1.0000\n",
            "Epoch: 15, Accuracy: 1.0000\n",
            "Epoch: 16, Accuracy: 1.0000\n",
            "Epoch: 17, Accuracy: 1.0000\n",
            "Epoch: 18, Accuracy: 1.0000\n",
            "Epoch: 19, Accuracy: 1.0000\n",
            "Epoch: 20, Accuracy: 1.0000\n",
            "Epoch: 21, Accuracy: 1.0000\n",
            "Epoch: 22, Accuracy: 1.0000\n",
            "Epoch: 23, Accuracy: 1.0000\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 512 hidden layers: 0.8050####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Parameter number of hidden layers: 1, 2, 4, 8, 16"
      ],
      "metadata": {
        "id": "m3RzN0CoATwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manually trying all different number of hidden layers\n",
        "\n"
      ],
      "metadata": {
        "id": "uwe1A3zaEC21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With 1 hidden layer"
      ],
      "metadata": {
        "id": "dtC3QZbkEKTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # you can change convolutions here\n",
        "        # 1 hidden layer\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, dataset.num_classes)\n",
        "        #self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = SAGEConv(16, dataset.num_classes)\n",
        "        # self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        # self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        # x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "    acc = correct / data.train_mask.sum().item()\n",
        "    print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))\n",
        "\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('####Test Accuracy with 1 hidden layer: {:.4f}####'.format(acc))"
      ],
      "metadata": {
        "id": "22-nx1VF5OWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63ceacf-c4e5-4331-98b5-c705ef4ef14b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.4857\n",
            "Epoch: 1, Accuracy: 0.6000\n",
            "Epoch: 2, Accuracy: 0.6786\n",
            "Epoch: 3, Accuracy: 0.8143\n",
            "Epoch: 4, Accuracy: 0.8643\n",
            "Epoch: 5, Accuracy: 0.9000\n",
            "Epoch: 6, Accuracy: 0.9357\n",
            "Epoch: 7, Accuracy: 0.9500\n",
            "Epoch: 8, Accuracy: 0.9714\n",
            "Epoch: 9, Accuracy: 0.9714\n",
            "Epoch: 10, Accuracy: 0.9714\n",
            "Epoch: 11, Accuracy: 0.9786\n",
            "Epoch: 12, Accuracy: 0.9786\n",
            "Epoch: 13, Accuracy: 0.9857\n",
            "Epoch: 14, Accuracy: 0.9857\n",
            "Epoch: 15, Accuracy: 0.9857\n",
            "Epoch: 16, Accuracy: 0.9929\n",
            "Epoch: 17, Accuracy: 0.9929\n",
            "Epoch: 18, Accuracy: 0.9929\n",
            "Epoch: 19, Accuracy: 0.9929\n",
            "Epoch: 20, Accuracy: 0.9929\n",
            "Epoch: 21, Accuracy: 0.9929\n",
            "Epoch: 22, Accuracy: 0.9929\n",
            "Epoch: 23, Accuracy: 0.9929\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 1 hidden layer: 0.7470####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With 2 hidden layers"
      ],
      "metadata": {
        "id": "v9YLy52JI4UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # you can change convolutions here\n",
        "        #self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = SAGEConv(16, dataset.num_classes)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "    acc = correct / data.train_mask.sum().item()\n",
        "    print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))\n",
        "\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('####Test Accuracy with 2 hidden layers: {:.4f}####'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny8-IJNTIx8X",
        "outputId": "c1fc0287-fae0-4734-a129-1ab3365b45f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.6000\n",
            "Epoch: 1, Accuracy: 0.7643\n",
            "Epoch: 2, Accuracy: 0.8143\n",
            "Epoch: 3, Accuracy: 0.8500\n",
            "Epoch: 4, Accuracy: 0.8714\n",
            "Epoch: 5, Accuracy: 0.9000\n",
            "Epoch: 6, Accuracy: 0.9143\n",
            "Epoch: 7, Accuracy: 0.9500\n",
            "Epoch: 8, Accuracy: 0.9643\n",
            "Epoch: 9, Accuracy: 0.9643\n",
            "Epoch: 10, Accuracy: 0.9786\n",
            "Epoch: 11, Accuracy: 0.9857\n",
            "Epoch: 12, Accuracy: 0.9857\n",
            "Epoch: 13, Accuracy: 0.9857\n",
            "Epoch: 14, Accuracy: 0.9857\n",
            "Epoch: 15, Accuracy: 0.9857\n",
            "Epoch: 16, Accuracy: 0.9929\n",
            "Epoch: 17, Accuracy: 1.0000\n",
            "Epoch: 18, Accuracy: 1.0000\n",
            "Epoch: 19, Accuracy: 1.0000\n",
            "Epoch: 20, Accuracy: 1.0000\n",
            "Epoch: 21, Accuracy: 1.0000\n",
            "Epoch: 22, Accuracy: 1.0000\n",
            "Epoch: 23, Accuracy: 1.0000\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 2 hidden layers: 0.8050####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With 4 hidden layers"
      ],
      "metadata": {
        "id": "EPDF141vJbmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # you can change convolutions here\n",
        "        #self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = SAGEConv(16, dataset.num_classes)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16,16)\n",
        "        self.conv3 = GCNConv(16,16)\n",
        "        self.conv4 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "    acc = correct / data.train_mask.sum().item()\n",
        "    print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))\n",
        "\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('####Test Accuracy with 4 hidden layers: {:.4f}####'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkotqSzQJim-",
        "outputId": "1b462bcc-07da-42eb-90d8-2ce757086d9a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.2857\n",
            "Epoch: 1, Accuracy: 0.4500\n",
            "Epoch: 2, Accuracy: 0.4429\n",
            "Epoch: 3, Accuracy: 0.4643\n",
            "Epoch: 4, Accuracy: 0.5214\n",
            "Epoch: 5, Accuracy: 0.5929\n",
            "Epoch: 6, Accuracy: 0.6643\n",
            "Epoch: 7, Accuracy: 0.6786\n",
            "Epoch: 8, Accuracy: 0.6786\n",
            "Epoch: 9, Accuracy: 0.7000\n",
            "Epoch: 10, Accuracy: 0.7071\n",
            "Epoch: 11, Accuracy: 0.7071\n",
            "Epoch: 12, Accuracy: 0.7071\n",
            "Epoch: 13, Accuracy: 0.7143\n",
            "Epoch: 14, Accuracy: 0.7357\n",
            "Epoch: 15, Accuracy: 0.8143\n",
            "Epoch: 16, Accuracy: 0.8786\n",
            "Epoch: 17, Accuracy: 0.9643\n",
            "Epoch: 18, Accuracy: 0.9786\n",
            "Epoch: 19, Accuracy: 0.9786\n",
            "Epoch: 20, Accuracy: 0.9929\n",
            "Epoch: 21, Accuracy: 0.9929\n",
            "Epoch: 22, Accuracy: 0.9929\n",
            "Epoch: 23, Accuracy: 0.9929\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with 4 hidden layers: 0.7700####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With 8 hidden layers"
      ],
      "metadata": {
        "id": "d_GaGPOmKgQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # you can change convolutions here\n",
        "        #self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = SAGEConv(16, dataset.num_classes)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16,16)\n",
        "        self.conv3 = GCNConv(16,16)\n",
        "        self.conv4 = GCNConv(16,16)\n",
        "        self.conv5 = GCNConv(16,16)\n",
        "        self.conv6 = GCNConv(16,16)\n",
        "        self.conv7 = GCNConv(16,16)\n",
        "        self.conv8 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv8(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "    acc = correct / data.train_mask.sum().item()\n",
        "    print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))\n",
        "\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('####Test Accuracy with 8 hidden layers: {:.4f}####'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyuKNAzGKghA",
        "outputId": "d662308f-8861-424e-d704-5b216ad5faa5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.1429\n",
            "Epoch: 1, Accuracy: 0.1500\n",
            "Epoch: 2, Accuracy: 0.1571\n",
            "Epoch: 3, Accuracy: 0.1714\n",
            "Epoch: 4, Accuracy: 0.1786\n",
            "Epoch: 5, Accuracy: 0.2143\n",
            "Epoch: 6, Accuracy: 0.2500\n",
            "Epoch: 7, Accuracy: 0.2571\n",
            "Epoch: 8, Accuracy: 0.2643\n",
            "Epoch: 9, Accuracy: 0.2857\n",
            "Epoch: 10, Accuracy: 0.2786\n",
            "Epoch: 11, Accuracy: 0.3500\n",
            "Epoch: 12, Accuracy: 0.3214\n",
            "Epoch: 13, Accuracy: 0.3357\n",
            "Epoch: 14, Accuracy: 0.4429\n",
            "Epoch: 15, Accuracy: 0.3286\n",
            "Epoch: 16, Accuracy: 0.4357\n",
            "Epoch: 17, Accuracy: 0.5071\n",
            "Epoch: 18, Accuracy: 0.4286\n",
            "Epoch: 19, Accuracy: 0.5429\n",
            "Epoch: 20, Accuracy: 0.5143\n",
            "Epoch: 21, Accuracy: 0.5357\n",
            "Epoch: 22, Accuracy: 0.5071\n",
            "Epoch: 23, Accuracy: 0.5929\n",
            "Epoch: 24, Accuracy: 0.6214\n",
            "Epoch: 25, Accuracy: 0.5714\n",
            "Epoch: 26, Accuracy: 0.6000\n",
            "Epoch: 27, Accuracy: 0.6143\n",
            "Epoch: 28, Accuracy: 0.6357\n",
            "Epoch: 29, Accuracy: 0.7214\n",
            "Epoch: 30, Accuracy: 0.6786\n",
            "Epoch: 31, Accuracy: 0.7857\n",
            "Epoch: 32, Accuracy: 0.7429\n",
            "Epoch: 33, Accuracy: 0.7786\n",
            "Epoch: 34, Accuracy: 0.8214\n",
            "Epoch: 35, Accuracy: 0.8143\n",
            "Epoch: 36, Accuracy: 0.8357\n",
            "Epoch: 37, Accuracy: 0.8714\n",
            "Epoch: 38, Accuracy: 0.8714\n",
            "Epoch: 39, Accuracy: 0.9000\n",
            "Epoch: 40, Accuracy: 0.8643\n",
            "Epoch: 41, Accuracy: 0.8214\n",
            "Epoch: 42, Accuracy: 0.7357\n",
            "Epoch: 43, Accuracy: 0.8286\n",
            "Epoch: 44, Accuracy: 0.7357\n",
            "Epoch: 45, Accuracy: 0.8571\n",
            "Epoch: 46, Accuracy: 0.6214\n",
            "Epoch: 47, Accuracy: 0.8643\n",
            "Epoch: 48, Accuracy: 0.6929\n",
            "Epoch: 49, Accuracy: 0.8786\n",
            "Epoch: 50, Accuracy: 0.9000\n",
            "Epoch: 51, Accuracy: 0.8143\n",
            "Epoch: 52, Accuracy: 0.7714\n",
            "Epoch: 53, Accuracy: 0.8357\n",
            "Epoch: 54, Accuracy: 0.9000\n",
            "Epoch: 55, Accuracy: 0.8929\n",
            "Epoch: 56, Accuracy: 0.8786\n",
            "Epoch: 57, Accuracy: 0.8357\n",
            "Epoch: 58, Accuracy: 0.8857\n",
            "Epoch: 59, Accuracy: 0.8857\n",
            "Epoch: 60, Accuracy: 0.8929\n",
            "Epoch: 61, Accuracy: 0.9071\n",
            "Epoch: 62, Accuracy: 0.9000\n",
            "Epoch: 63, Accuracy: 0.9000\n",
            "Epoch: 64, Accuracy: 0.8929\n",
            "Epoch: 65, Accuracy: 0.9000\n",
            "Epoch: 66, Accuracy: 0.9143\n",
            "Epoch: 67, Accuracy: 0.9143\n",
            "Epoch: 68, Accuracy: 0.9286\n",
            "Epoch: 69, Accuracy: 0.9286\n",
            "Epoch: 70, Accuracy: 0.9500\n",
            "Epoch: 71, Accuracy: 0.9500\n",
            "Epoch: 72, Accuracy: 0.9429\n",
            "Epoch: 73, Accuracy: 0.9357\n",
            "Epoch: 74, Accuracy: 0.9571\n",
            "Epoch: 75, Accuracy: 0.9571\n",
            "Epoch: 76, Accuracy: 0.9571\n",
            "Epoch: 77, Accuracy: 0.9571\n",
            "Epoch: 78, Accuracy: 0.9571\n",
            "Epoch: 79, Accuracy: 0.9643\n",
            "Epoch: 80, Accuracy: 0.9714\n",
            "Epoch: 81, Accuracy: 0.9714\n",
            "Epoch: 82, Accuracy: 0.9714\n",
            "Epoch: 83, Accuracy: 0.9786\n",
            "Epoch: 84, Accuracy: 0.9786\n",
            "Epoch: 85, Accuracy: 0.9786\n",
            "Epoch: 86, Accuracy: 0.9786\n",
            "Epoch: 87, Accuracy: 0.9857\n",
            "Epoch: 88, Accuracy: 0.9857\n",
            "Epoch: 89, Accuracy: 0.9857\n",
            "Epoch: 90, Accuracy: 0.9857\n",
            "Epoch: 91, Accuracy: 0.9857\n",
            "Epoch: 92, Accuracy: 0.9857\n",
            "Epoch: 93, Accuracy: 0.9857\n",
            "Epoch: 94, Accuracy: 0.9857\n",
            "Epoch: 95, Accuracy: 0.9857\n",
            "Epoch: 96, Accuracy: 0.9857\n",
            "Epoch: 97, Accuracy: 0.9857\n",
            "Epoch: 98, Accuracy: 0.9929\n",
            "Epoch: 99, Accuracy: 0.9857\n",
            "####Test Accuracy with 8 hidden layers: 0.6110####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With 16 hidden layers"
      ],
      "metadata": {
        "id": "yRwkerDQLYR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # you can change convolutions here\n",
        "        #self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = SAGEConv(16, dataset.num_classes)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16,16)\n",
        "        self.conv3 = GCNConv(16,16)\n",
        "        self.conv4 = GCNConv(16,16)\n",
        "        self.conv5 = GCNConv(16,16)\n",
        "        self.conv6 = GCNConv(16,16)\n",
        "        self.conv7 = GCNConv(16,16)\n",
        "        self.conv8 = GCNConv(16,16)\n",
        "        self.conv9 = GCNConv(16,16)\n",
        "        self.conv10 = GCNConv(16,16)\n",
        "        self.conv11 = GCNConv(16,16)\n",
        "        self.conv12 = GCNConv(16,16)\n",
        "        self.conv13 = GCNConv(16,16)\n",
        "        self.conv14 = GCNConv(16,16)\n",
        "        self.conv15 = GCNConv(16,16)\n",
        "        self.conv16 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv8(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv9(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv10(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv11(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv12(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv13(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv14(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv15(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv16(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "    acc = correct / data.train_mask.sum().item()\n",
        "    print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))\n",
        "\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('####Test Accuracy with 16 hidden layers: {:.4f}####'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqbnYrAjLcIo",
        "outputId": "691b7a2a-934b-4c8f-e85b-ac716e3f60fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Accuracy: 0.1500\n",
            "Epoch: 1, Accuracy: 0.1429\n",
            "Epoch: 2, Accuracy: 0.1571\n",
            "Epoch: 3, Accuracy: 0.1786\n",
            "Epoch: 4, Accuracy: 0.1571\n",
            "Epoch: 5, Accuracy: 0.1357\n",
            "Epoch: 6, Accuracy: 0.1500\n",
            "Epoch: 7, Accuracy: 0.1571\n",
            "Epoch: 8, Accuracy: 0.1214\n",
            "Epoch: 9, Accuracy: 0.1429\n",
            "Epoch: 10, Accuracy: 0.1286\n",
            "Epoch: 11, Accuracy: 0.1286\n",
            "Epoch: 12, Accuracy: 0.1286\n",
            "Epoch: 13, Accuracy: 0.1357\n",
            "Epoch: 14, Accuracy: 0.1429\n",
            "Epoch: 15, Accuracy: 0.1429\n",
            "Epoch: 16, Accuracy: 0.1500\n",
            "Epoch: 17, Accuracy: 0.1500\n",
            "Epoch: 18, Accuracy: 0.1429\n",
            "Epoch: 19, Accuracy: 0.1429\n",
            "Epoch: 20, Accuracy: 0.1357\n",
            "Epoch: 21, Accuracy: 0.1571\n",
            "Epoch: 22, Accuracy: 0.1571\n",
            "Epoch: 23, Accuracy: 0.1643\n",
            "Epoch: 24, Accuracy: 0.1571\n",
            "Epoch: 25, Accuracy: 0.1643\n",
            "Epoch: 26, Accuracy: 0.1643\n",
            "Epoch: 27, Accuracy: 0.1643\n",
            "Epoch: 28, Accuracy: 0.1714\n",
            "Epoch: 29, Accuracy: 0.1714\n",
            "Epoch: 30, Accuracy: 0.1714\n",
            "Epoch: 31, Accuracy: 0.1857\n",
            "Epoch: 32, Accuracy: 0.1857\n",
            "Epoch: 33, Accuracy: 0.1857\n",
            "Epoch: 34, Accuracy: 0.1857\n",
            "Epoch: 35, Accuracy: 0.1643\n",
            "Epoch: 36, Accuracy: 0.1929\n",
            "Epoch: 37, Accuracy: 0.1786\n",
            "Epoch: 38, Accuracy: 0.1643\n",
            "Epoch: 39, Accuracy: 0.1786\n",
            "Epoch: 40, Accuracy: 0.1786\n",
            "Epoch: 41, Accuracy: 0.1571\n",
            "Epoch: 42, Accuracy: 0.1571\n",
            "Epoch: 43, Accuracy: 0.1500\n",
            "Epoch: 44, Accuracy: 0.1571\n",
            "Epoch: 45, Accuracy: 0.1571\n",
            "Epoch: 46, Accuracy: 0.1643\n",
            "Epoch: 47, Accuracy: 0.1714\n",
            "Epoch: 48, Accuracy: 0.1571\n",
            "Epoch: 49, Accuracy: 0.2071\n",
            "Epoch: 50, Accuracy: 0.1857\n",
            "Epoch: 51, Accuracy: 0.1571\n",
            "Epoch: 52, Accuracy: 0.1571\n",
            "Epoch: 53, Accuracy: 0.1714\n",
            "Epoch: 54, Accuracy: 0.1857\n",
            "Epoch: 55, Accuracy: 0.1929\n",
            "Epoch: 56, Accuracy: 0.1571\n",
            "Epoch: 57, Accuracy: 0.1714\n",
            "Epoch: 58, Accuracy: 0.1929\n",
            "Epoch: 59, Accuracy: 0.1929\n",
            "Epoch: 60, Accuracy: 0.1500\n",
            "Epoch: 61, Accuracy: 0.2000\n",
            "Epoch: 62, Accuracy: 0.1714\n",
            "Epoch: 63, Accuracy: 0.1714\n",
            "Epoch: 64, Accuracy: 0.2071\n",
            "Epoch: 65, Accuracy: 0.2286\n",
            "Epoch: 66, Accuracy: 0.2357\n",
            "Epoch: 67, Accuracy: 0.1786\n",
            "Epoch: 68, Accuracy: 0.2143\n",
            "Epoch: 69, Accuracy: 0.1929\n",
            "Epoch: 70, Accuracy: 0.2214\n",
            "Epoch: 71, Accuracy: 0.1714\n",
            "Epoch: 72, Accuracy: 0.2214\n",
            "Epoch: 73, Accuracy: 0.2357\n",
            "Epoch: 74, Accuracy: 0.2500\n",
            "Epoch: 75, Accuracy: 0.2143\n",
            "Epoch: 76, Accuracy: 0.2143\n",
            "Epoch: 77, Accuracy: 0.1786\n",
            "Epoch: 78, Accuracy: 0.1857\n",
            "Epoch: 79, Accuracy: 0.2143\n",
            "Epoch: 80, Accuracy: 0.2500\n",
            "Epoch: 81, Accuracy: 0.2214\n",
            "Epoch: 82, Accuracy: 0.2429\n",
            "Epoch: 83, Accuracy: 0.2429\n",
            "Epoch: 84, Accuracy: 0.2429\n",
            "Epoch: 85, Accuracy: 0.2071\n",
            "Epoch: 86, Accuracy: 0.2286\n",
            "Epoch: 87, Accuracy: 0.2071\n",
            "Epoch: 88, Accuracy: 0.2071\n",
            "Epoch: 89, Accuracy: 0.2429\n",
            "Epoch: 90, Accuracy: 0.2071\n",
            "Epoch: 91, Accuracy: 0.1857\n",
            "Epoch: 92, Accuracy: 0.2571\n",
            "Epoch: 93, Accuracy: 0.2286\n",
            "Epoch: 94, Accuracy: 0.2071\n",
            "Epoch: 95, Accuracy: 0.2571\n",
            "Epoch: 96, Accuracy: 0.2357\n",
            "Epoch: 97, Accuracy: 0.2143\n",
            "Epoch: 98, Accuracy: 0.2500\n",
            "Epoch: 99, Accuracy: 0.2357\n",
            "####Test Accuracy with 16 hidden layers: 0.1440####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Parameter learning rate: 1, .5, .1, .01, .0001"
      ],
      "metadata": {
        "id": "aHR1xlB-MmYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # you can change convolutions here\n",
        "        #self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = SAGEConv(16, dataset.num_classes)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "7lJfDTTnM787"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "learning_rates = [1, .5, .1, .01, .0001]\n",
        "\n",
        "for rate in learning_rates:\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=rate, weight_decay=5e-4)\n",
        "  model.train()\n",
        "  print(f\"####Training GCN with a learning rate of {rate}...####\")\n",
        "  for epoch in range(100):\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data)\n",
        "      loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      _, pred = model(data).max(dim=1)\n",
        "      correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "      acc = correct / data.train_mask.sum().item()\n",
        "      print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))\n",
        "\n",
        "\n",
        "  _, pred = model(data).max(dim=1)\n",
        "  correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "  acc = correct / data.test_mask.sum().item()\n",
        "  print('####Test Accuracy with a learning rate of', rate, 'is: {:.4f}####'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqriG6EyNDsU",
        "outputId": "fc6c89a8-1482-4cd6-f220-90f0e13dac25"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####Training GCN with a learning rate of 1...####\n",
            "Epoch: 0, Accuracy: 0.1643\n",
            "Epoch: 1, Accuracy: 0.2714\n",
            "Epoch: 2, Accuracy: 0.4286\n",
            "Epoch: 3, Accuracy: 0.3929\n",
            "Epoch: 4, Accuracy: 0.4500\n",
            "Epoch: 5, Accuracy: 0.5357\n",
            "Epoch: 6, Accuracy: 0.5714\n",
            "Epoch: 7, Accuracy: 0.6286\n",
            "Epoch: 8, Accuracy: 0.6071\n",
            "Epoch: 9, Accuracy: 0.6071\n",
            "Epoch: 10, Accuracy: 0.6429\n",
            "Epoch: 11, Accuracy: 0.6714\n",
            "Epoch: 12, Accuracy: 0.6786\n",
            "Epoch: 13, Accuracy: 0.5786\n",
            "Epoch: 14, Accuracy: 0.6929\n",
            "Epoch: 15, Accuracy: 0.6857\n",
            "Epoch: 16, Accuracy: 0.6786\n",
            "Epoch: 17, Accuracy: 0.6786\n",
            "Epoch: 18, Accuracy: 0.6857\n",
            "Epoch: 19, Accuracy: 0.6714\n",
            "Epoch: 20, Accuracy: 0.6786\n",
            "Epoch: 21, Accuracy: 0.6857\n",
            "Epoch: 22, Accuracy: 0.6286\n",
            "Epoch: 23, Accuracy: 0.6929\n",
            "Epoch: 24, Accuracy: 0.7000\n",
            "Epoch: 25, Accuracy: 0.7286\n",
            "Epoch: 26, Accuracy: 0.7143\n",
            "Epoch: 27, Accuracy: 0.7286\n",
            "Epoch: 28, Accuracy: 0.7286\n",
            "Epoch: 29, Accuracy: 0.7214\n",
            "Epoch: 30, Accuracy: 0.7286\n",
            "Epoch: 31, Accuracy: 0.7286\n",
            "Epoch: 32, Accuracy: 0.7071\n",
            "Epoch: 33, Accuracy: 0.7286\n",
            "Epoch: 34, Accuracy: 0.7286\n",
            "Epoch: 35, Accuracy: 0.7357\n",
            "Epoch: 36, Accuracy: 0.7500\n",
            "Epoch: 37, Accuracy: 0.7500\n",
            "Epoch: 38, Accuracy: 0.7714\n",
            "Epoch: 39, Accuracy: 0.7857\n",
            "Epoch: 40, Accuracy: 0.7786\n",
            "Epoch: 41, Accuracy: 0.7929\n",
            "Epoch: 42, Accuracy: 0.7929\n",
            "Epoch: 43, Accuracy: 0.7714\n",
            "Epoch: 44, Accuracy: 0.8071\n",
            "Epoch: 45, Accuracy: 0.8071\n",
            "Epoch: 46, Accuracy: 0.8214\n",
            "Epoch: 47, Accuracy: 0.8071\n",
            "Epoch: 48, Accuracy: 0.8286\n",
            "Epoch: 49, Accuracy: 0.8214\n",
            "Epoch: 50, Accuracy: 0.8357\n",
            "Epoch: 51, Accuracy: 0.8500\n",
            "Epoch: 52, Accuracy: 0.8143\n",
            "Epoch: 53, Accuracy: 0.8429\n",
            "Epoch: 54, Accuracy: 0.8643\n",
            "Epoch: 55, Accuracy: 0.8786\n",
            "Epoch: 56, Accuracy: 0.8929\n",
            "Epoch: 57, Accuracy: 0.8714\n",
            "Epoch: 58, Accuracy: 0.8786\n",
            "Epoch: 59, Accuracy: 0.8929\n",
            "Epoch: 60, Accuracy: 0.8929\n",
            "Epoch: 61, Accuracy: 0.8857\n",
            "Epoch: 62, Accuracy: 0.8929\n",
            "Epoch: 63, Accuracy: 0.8857\n",
            "Epoch: 64, Accuracy: 0.8857\n",
            "Epoch: 65, Accuracy: 0.8929\n",
            "Epoch: 66, Accuracy: 0.8857\n",
            "Epoch: 67, Accuracy: 0.9143\n",
            "Epoch: 68, Accuracy: 0.8857\n",
            "Epoch: 69, Accuracy: 0.8714\n",
            "Epoch: 70, Accuracy: 0.8929\n",
            "Epoch: 71, Accuracy: 0.8786\n",
            "Epoch: 72, Accuracy: 0.9000\n",
            "Epoch: 73, Accuracy: 0.8786\n",
            "Epoch: 74, Accuracy: 0.8929\n",
            "Epoch: 75, Accuracy: 0.8714\n",
            "Epoch: 76, Accuracy: 0.8857\n",
            "Epoch: 77, Accuracy: 0.8643\n",
            "Epoch: 78, Accuracy: 0.9071\n",
            "Epoch: 79, Accuracy: 0.8857\n",
            "Epoch: 80, Accuracy: 0.8714\n",
            "Epoch: 81, Accuracy: 0.9143\n",
            "Epoch: 82, Accuracy: 0.9286\n",
            "Epoch: 83, Accuracy: 0.9143\n",
            "Epoch: 84, Accuracy: 0.9286\n",
            "Epoch: 85, Accuracy: 0.9286\n",
            "Epoch: 86, Accuracy: 0.9357\n",
            "Epoch: 87, Accuracy: 0.9286\n",
            "Epoch: 88, Accuracy: 0.9500\n",
            "Epoch: 89, Accuracy: 0.9643\n",
            "Epoch: 90, Accuracy: 0.9500\n",
            "Epoch: 91, Accuracy: 0.9643\n",
            "Epoch: 92, Accuracy: 0.9500\n",
            "Epoch: 93, Accuracy: 0.9643\n",
            "Epoch: 94, Accuracy: 0.9714\n",
            "Epoch: 95, Accuracy: 0.9643\n",
            "Epoch: 96, Accuracy: 0.9714\n",
            "Epoch: 97, Accuracy: 0.9643\n",
            "Epoch: 98, Accuracy: 0.9714\n",
            "Epoch: 99, Accuracy: 0.9786\n",
            "####Test Accuracy with a learning rate of 1 is: 0.6420####\n",
            "####Training GCN with a learning rate of 0.5...####\n",
            "Epoch: 0, Accuracy: 0.3571\n",
            "Epoch: 1, Accuracy: 0.3500\n",
            "Epoch: 2, Accuracy: 0.5357\n",
            "Epoch: 3, Accuracy: 0.5357\n",
            "Epoch: 4, Accuracy: 0.4929\n",
            "Epoch: 5, Accuracy: 0.4857\n",
            "Epoch: 6, Accuracy: 0.6214\n",
            "Epoch: 7, Accuracy: 0.5929\n",
            "Epoch: 8, Accuracy: 0.6500\n",
            "Epoch: 9, Accuracy: 0.7429\n",
            "Epoch: 10, Accuracy: 0.7929\n",
            "Epoch: 11, Accuracy: 0.8143\n",
            "Epoch: 12, Accuracy: 0.8429\n",
            "Epoch: 13, Accuracy: 0.8786\n",
            "Epoch: 14, Accuracy: 0.8714\n",
            "Epoch: 15, Accuracy: 0.8857\n",
            "Epoch: 16, Accuracy: 0.8857\n",
            "Epoch: 17, Accuracy: 0.8286\n",
            "Epoch: 18, Accuracy: 0.8286\n",
            "Epoch: 19, Accuracy: 0.8357\n",
            "Epoch: 20, Accuracy: 0.8500\n",
            "Epoch: 21, Accuracy: 0.8500\n",
            "Epoch: 22, Accuracy: 0.8571\n",
            "Epoch: 23, Accuracy: 0.9357\n",
            "Epoch: 24, Accuracy: 0.9429\n",
            "Epoch: 25, Accuracy: 0.9429\n",
            "Epoch: 26, Accuracy: 0.9429\n",
            "Epoch: 27, Accuracy: 0.9571\n",
            "Epoch: 28, Accuracy: 0.9571\n",
            "Epoch: 29, Accuracy: 0.9571\n",
            "Epoch: 30, Accuracy: 0.9429\n",
            "Epoch: 31, Accuracy: 0.9571\n",
            "Epoch: 32, Accuracy: 0.9357\n",
            "Epoch: 33, Accuracy: 0.9143\n",
            "Epoch: 34, Accuracy: 0.9643\n",
            "Epoch: 35, Accuracy: 0.9429\n",
            "Epoch: 36, Accuracy: 0.9500\n",
            "Epoch: 37, Accuracy: 0.9571\n",
            "Epoch: 38, Accuracy: 0.9714\n",
            "Epoch: 39, Accuracy: 0.9714\n",
            "Epoch: 40, Accuracy: 0.9786\n",
            "Epoch: 41, Accuracy: 0.9857\n",
            "Epoch: 42, Accuracy: 0.9929\n",
            "Epoch: 43, Accuracy: 0.9786\n",
            "Epoch: 44, Accuracy: 0.9857\n",
            "Epoch: 45, Accuracy: 0.9857\n",
            "Epoch: 46, Accuracy: 0.9857\n",
            "Epoch: 47, Accuracy: 0.9857\n",
            "Epoch: 48, Accuracy: 0.9929\n",
            "Epoch: 49, Accuracy: 0.9929\n",
            "Epoch: 50, Accuracy: 0.9929\n",
            "Epoch: 51, Accuracy: 0.9929\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 0.9929\n",
            "Epoch: 54, Accuracy: 0.9929\n",
            "Epoch: 55, Accuracy: 0.9929\n",
            "Epoch: 56, Accuracy: 0.9929\n",
            "Epoch: 57, Accuracy: 0.9929\n",
            "Epoch: 58, Accuracy: 0.9929\n",
            "Epoch: 59, Accuracy: 0.9929\n",
            "Epoch: 60, Accuracy: 0.9929\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with a learning rate of 0.5 is: 0.7750####\n",
            "####Training GCN with a learning rate of 0.1...####\n",
            "Epoch: 0, Accuracy: 0.8643\n",
            "Epoch: 1, Accuracy: 0.9857\n",
            "Epoch: 2, Accuracy: 0.9500\n",
            "Epoch: 3, Accuracy: 1.0000\n",
            "Epoch: 4, Accuracy: 1.0000\n",
            "Epoch: 5, Accuracy: 1.0000\n",
            "Epoch: 6, Accuracy: 1.0000\n",
            "Epoch: 7, Accuracy: 1.0000\n",
            "Epoch: 8, Accuracy: 1.0000\n",
            "Epoch: 9, Accuracy: 1.0000\n",
            "Epoch: 10, Accuracy: 1.0000\n",
            "Epoch: 11, Accuracy: 1.0000\n",
            "Epoch: 12, Accuracy: 1.0000\n",
            "Epoch: 13, Accuracy: 1.0000\n",
            "Epoch: 14, Accuracy: 1.0000\n",
            "Epoch: 15, Accuracy: 1.0000\n",
            "Epoch: 16, Accuracy: 1.0000\n",
            "Epoch: 17, Accuracy: 1.0000\n",
            "Epoch: 18, Accuracy: 1.0000\n",
            "Epoch: 19, Accuracy: 1.0000\n",
            "Epoch: 20, Accuracy: 1.0000\n",
            "Epoch: 21, Accuracy: 1.0000\n",
            "Epoch: 22, Accuracy: 1.0000\n",
            "Epoch: 23, Accuracy: 1.0000\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with a learning rate of 0.1 is: 0.8030####\n",
            "####Training GCN with a learning rate of 0.01...####\n",
            "Epoch: 0, Accuracy: 1.0000\n",
            "Epoch: 1, Accuracy: 1.0000\n",
            "Epoch: 2, Accuracy: 1.0000\n",
            "Epoch: 3, Accuracy: 1.0000\n",
            "Epoch: 4, Accuracy: 1.0000\n",
            "Epoch: 5, Accuracy: 1.0000\n",
            "Epoch: 6, Accuracy: 1.0000\n",
            "Epoch: 7, Accuracy: 1.0000\n",
            "Epoch: 8, Accuracy: 1.0000\n",
            "Epoch: 9, Accuracy: 1.0000\n",
            "Epoch: 10, Accuracy: 1.0000\n",
            "Epoch: 11, Accuracy: 1.0000\n",
            "Epoch: 12, Accuracy: 1.0000\n",
            "Epoch: 13, Accuracy: 1.0000\n",
            "Epoch: 14, Accuracy: 1.0000\n",
            "Epoch: 15, Accuracy: 1.0000\n",
            "Epoch: 16, Accuracy: 1.0000\n",
            "Epoch: 17, Accuracy: 1.0000\n",
            "Epoch: 18, Accuracy: 1.0000\n",
            "Epoch: 19, Accuracy: 1.0000\n",
            "Epoch: 20, Accuracy: 1.0000\n",
            "Epoch: 21, Accuracy: 1.0000\n",
            "Epoch: 22, Accuracy: 1.0000\n",
            "Epoch: 23, Accuracy: 1.0000\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with a learning rate of 0.01 is: 0.8040####\n",
            "####Training GCN with a learning rate of 0.0001...####\n",
            "Epoch: 0, Accuracy: 1.0000\n",
            "Epoch: 1, Accuracy: 1.0000\n",
            "Epoch: 2, Accuracy: 1.0000\n",
            "Epoch: 3, Accuracy: 1.0000\n",
            "Epoch: 4, Accuracy: 1.0000\n",
            "Epoch: 5, Accuracy: 1.0000\n",
            "Epoch: 6, Accuracy: 1.0000\n",
            "Epoch: 7, Accuracy: 1.0000\n",
            "Epoch: 8, Accuracy: 1.0000\n",
            "Epoch: 9, Accuracy: 1.0000\n",
            "Epoch: 10, Accuracy: 1.0000\n",
            "Epoch: 11, Accuracy: 1.0000\n",
            "Epoch: 12, Accuracy: 1.0000\n",
            "Epoch: 13, Accuracy: 1.0000\n",
            "Epoch: 14, Accuracy: 1.0000\n",
            "Epoch: 15, Accuracy: 1.0000\n",
            "Epoch: 16, Accuracy: 1.0000\n",
            "Epoch: 17, Accuracy: 1.0000\n",
            "Epoch: 18, Accuracy: 1.0000\n",
            "Epoch: 19, Accuracy: 1.0000\n",
            "Epoch: 20, Accuracy: 1.0000\n",
            "Epoch: 21, Accuracy: 1.0000\n",
            "Epoch: 22, Accuracy: 1.0000\n",
            "Epoch: 23, Accuracy: 1.0000\n",
            "Epoch: 24, Accuracy: 1.0000\n",
            "Epoch: 25, Accuracy: 1.0000\n",
            "Epoch: 26, Accuracy: 1.0000\n",
            "Epoch: 27, Accuracy: 1.0000\n",
            "Epoch: 28, Accuracy: 1.0000\n",
            "Epoch: 29, Accuracy: 1.0000\n",
            "Epoch: 30, Accuracy: 1.0000\n",
            "Epoch: 31, Accuracy: 1.0000\n",
            "Epoch: 32, Accuracy: 1.0000\n",
            "Epoch: 33, Accuracy: 1.0000\n",
            "Epoch: 34, Accuracy: 1.0000\n",
            "Epoch: 35, Accuracy: 1.0000\n",
            "Epoch: 36, Accuracy: 1.0000\n",
            "Epoch: 37, Accuracy: 1.0000\n",
            "Epoch: 38, Accuracy: 1.0000\n",
            "Epoch: 39, Accuracy: 1.0000\n",
            "Epoch: 40, Accuracy: 1.0000\n",
            "Epoch: 41, Accuracy: 1.0000\n",
            "Epoch: 42, Accuracy: 1.0000\n",
            "Epoch: 43, Accuracy: 1.0000\n",
            "Epoch: 44, Accuracy: 1.0000\n",
            "Epoch: 45, Accuracy: 1.0000\n",
            "Epoch: 46, Accuracy: 1.0000\n",
            "Epoch: 47, Accuracy: 1.0000\n",
            "Epoch: 48, Accuracy: 1.0000\n",
            "Epoch: 49, Accuracy: 1.0000\n",
            "Epoch: 50, Accuracy: 1.0000\n",
            "Epoch: 51, Accuracy: 1.0000\n",
            "Epoch: 52, Accuracy: 1.0000\n",
            "Epoch: 53, Accuracy: 1.0000\n",
            "Epoch: 54, Accuracy: 1.0000\n",
            "Epoch: 55, Accuracy: 1.0000\n",
            "Epoch: 56, Accuracy: 1.0000\n",
            "Epoch: 57, Accuracy: 1.0000\n",
            "Epoch: 58, Accuracy: 1.0000\n",
            "Epoch: 59, Accuracy: 1.0000\n",
            "Epoch: 60, Accuracy: 1.0000\n",
            "Epoch: 61, Accuracy: 1.0000\n",
            "Epoch: 62, Accuracy: 1.0000\n",
            "Epoch: 63, Accuracy: 1.0000\n",
            "Epoch: 64, Accuracy: 1.0000\n",
            "Epoch: 65, Accuracy: 1.0000\n",
            "Epoch: 66, Accuracy: 1.0000\n",
            "Epoch: 67, Accuracy: 1.0000\n",
            "Epoch: 68, Accuracy: 1.0000\n",
            "Epoch: 69, Accuracy: 1.0000\n",
            "Epoch: 70, Accuracy: 1.0000\n",
            "Epoch: 71, Accuracy: 1.0000\n",
            "Epoch: 72, Accuracy: 1.0000\n",
            "Epoch: 73, Accuracy: 1.0000\n",
            "Epoch: 74, Accuracy: 1.0000\n",
            "Epoch: 75, Accuracy: 1.0000\n",
            "Epoch: 76, Accuracy: 1.0000\n",
            "Epoch: 77, Accuracy: 1.0000\n",
            "Epoch: 78, Accuracy: 1.0000\n",
            "Epoch: 79, Accuracy: 1.0000\n",
            "Epoch: 80, Accuracy: 1.0000\n",
            "Epoch: 81, Accuracy: 1.0000\n",
            "Epoch: 82, Accuracy: 1.0000\n",
            "Epoch: 83, Accuracy: 1.0000\n",
            "Epoch: 84, Accuracy: 1.0000\n",
            "Epoch: 85, Accuracy: 1.0000\n",
            "Epoch: 86, Accuracy: 1.0000\n",
            "Epoch: 87, Accuracy: 1.0000\n",
            "Epoch: 88, Accuracy: 1.0000\n",
            "Epoch: 89, Accuracy: 1.0000\n",
            "Epoch: 90, Accuracy: 1.0000\n",
            "Epoch: 91, Accuracy: 1.0000\n",
            "Epoch: 92, Accuracy: 1.0000\n",
            "Epoch: 93, Accuracy: 1.0000\n",
            "Epoch: 94, Accuracy: 1.0000\n",
            "Epoch: 95, Accuracy: 1.0000\n",
            "Epoch: 96, Accuracy: 1.0000\n",
            "Epoch: 97, Accuracy: 1.0000\n",
            "Epoch: 98, Accuracy: 1.0000\n",
            "Epoch: 99, Accuracy: 1.0000\n",
            "####Test Accuracy with a learning rate of 0.0001 is: 0.8040####\n"
          ]
        }
      ]
    }
  ]
}